s1=s2-19
temp = big_data[(big_data$k >= s1[datnum] & big_data$k <= s2[datnum]),]
## Set working directory
setwd("G:/BACKUP/Chris/Project_3/Main simulations/Beta prior/Small n/Fixed/4 categories/Skewed/Moderate effect")
# For simplicity assume fixed design, n = 50, OR = 1.50, J = 4, skewed probs
## Main simulation scenario
set.seed(300524)
n = 100
nsim = 1000
## Generate simulation inputs
OR      <- c(1/1.50) # Assume these PORs
logOR   <- log(OR)                     # Take their log.
sdlogOR <- c(0) # No departure from PO
# Munge simulation scenarios
scen <- expand.grid("logOR" = logOR)
## Define states and other parameters
# 4 category outcome
states <- c("A", "B", "C","D")
iStates <- seq(states)
# Control probs in higher categories
quant <- pbeta(c(1/length(states) * seq(1,length(states)-1,1)),1.5,2.5)
p0 <- c(diff(c(0,quant)),1-quant[length(states)-1])
## Load in the appropriate packages
library("truncnorm")
## Define functions
getProbs <- function(p0 = c(10, 40, 10,40)/100,
logOddsRatio, sdLogOR)
{
cumProbs0   <- cumsum(p0)
cumOdds0    <- cumProbs0/(1 - cumProbs0)
cumlogOdds0 <- log(cumOdds0)
cumlogOdds1 <- rep(NA,length(p0))
cumlogOdds1[length(p0)] <- Inf
for(i in (length(p0) - 1):1){
cumlogOdds1[i] <- cumlogOdds0[i] + rtruncnorm(n=1,b=(cumlogOdds1[i+1]-cumlogOdds0[i]),mean=logOddsRatio,
sd=sdLogOR)
}
cumOdds1    <- exp(cumlogOdds1)
cumProbs1   <- cumOdds1/(1 + cumOdds1)
cumProbs1[length(cumProbs1)] <- 1
p1          <- diff(c(0,cumProbs1))
p1
## Any negative probabilities?
for(i in 1:length(p1)){
if(p1[i] < 0)
stop("Probabilities are negative")
}
p1
}
## Simulate a trial function
simulateTrial <- function(logOddsRatio,
nSamples0,
nSamples1,
sdLogOR,
p0     = c(20, 40, 30,10)/100,
states = c("A", "B", "C","D"))
{
## Define outcome probabilities for the treatment group
p1   <- getProbs(p0, logOddsRatio, sdLogOR)
p1
# Simulate two arm trial data.
dMulti0  <- rmultinom(1, size = nSamples0, prob = p0)
dMulti1  <- rmultinom(1, size = nSamples1, prob = p1)
sample0  <- rep(states, dMulti0)
sample1  <- rep(states, dMulti1)
sample0  <- factor(sample0, levels = states, ordered = T)
sample1  <- factor(sample1, levels = states, ordered = T)
# Munge simulated data.
data           <- rbind(data.frame("a" = 0, "y" = sample0),
data.frame("a" = 1, "y" = sample1))
data
}
## Function to make stan inputs.
makeStanData <- function(dat)
{
z <- data.frame("y" = as.numeric(dat[["y"]]),
"X" = as.numeric(dat[["a"]]))
z <- z[["X"]] == 1 & z[["y"]] > 2
stanData <- list(N = nrow(dat),
y = as.numeric(dat[["y"]]),
X = as.matrix((dat[["a"]])),
Z = as.matrix(as.numeric(z)),
p = 1,
q = 1,
k = length(states))
}
# Main simulation routine
random_number_generator_seeds <- matrix(0,nrow=nsim,ncol=626) # stores the random number generator seeds
# Store full dataset
df <- list()
for(k in 1:nsim){
# Save the random number generator seed
random_number_generator_seeds[k,] <- .Random.seed
## Simulate the data
# Sample size in each group
randtrt <- rbinom(n,1,c(0.5,0.5))  ## Ber(0.5)
sampsize0 <- table(randtrt)[1]
sampsize1 <- table(randtrt)[2]
simData <- list()
for(i in seq(nrow(scen)))
{
dat <- list()
for(j in seq(1))
{
print(c(i,j))
dat[[j]] <- simulateTrial(nSamples0 = sampsize0,
nSamples1 = sampsize1,
p0     = p0,
states = states,
logOddsRatio = with(scen,logOR[i]),
sdLogOR = with(scen,sdlogOR[i]))
}
simData[[i]] <- dat
}
# Make a single dataset
singleDat <- makeStanData(simData[[1]][[1]])
dat <- data.frame(x=singleDat$X[,], y = singleDat$y, x1 = as.factor(singleDat$X[,]))
dat$k <- k
df[[k]] <- dat
}
big_data = do.call(rbind,df)
save(big_data, file = "sim_data.Rdata")
save(random_number_generator_seeds, file = "seeds.Rdata")
args = commandArgs(trailingOnly = TRUE)
datnum <- as.numeric(args[1])
# Load the 1000 datasets
load("sim_data.Rdata")
attach(big_data)
# Select a subset of the data
s2 = seq(20,1000,by=20)
s1=s2-19
temp = big_data[(big_data$k >= s1[datnum] & big_data$k <= s2[datnum]),]
# Set seed
set.seed(071023)
seed=sample(1:1e8,50,replace=F)[datnum]
setwd("G:/BACKUP/Chris/Project_3/Main_simulations/Beta_prior/Small_n/Fixed/4_categories/Skewed/Moderate_effect")
# Load the 1000 datasets
load("sim_data.Rdata")
attach(big_data)
# Select a subset of the data
s2 = seq(20,1000,by=20)
s1=s2-19
temp = big_data[(big_data$k >= s1[datnum] & big_data$k <= s2[datnum]),]
# Set seed
set.seed(071023)
seed=sample(1:1e8,50,replace=F)[datnum]
nsim <- 1000
## Define states and other parameters
# 4 category outcome
states <- c("A", "B", "C","D")
#Generate empty vectors to store performance measures and diagnostic measures
## Store bias for each analysis model
bias_normal_largesd <- bias_normal_smallsd <- bias_laplace_largesd <- bias_laplace_smallsd <- bias_cauchy <- bias_rsquare <- length(nsim)
## Store coverage for each analysis model
coverage_normal_largesd <- coverage_normal_smallsd <- coverage_laplace_largesd <- coverage_laplace_smallsd <- coverage_cauchy <- coverage_rsquare <- length(nsim)
## Store MSE for each analysis model
mse_normal_largesd <- mse_normal_smallsd <- mse_laplace_largesd <- mse_laplace_smallsd <- mse_cauchy <- mse_rsquare <- length(nsim)
## Store R-hat for each parameter of each cumulative logit for each analysis model
rhat_normal_largesd <- rhat_normal_smallsd <- rhat_laplace_largesd <- rhat_laplace_smallsd <- rhat_cauchy <- rhat_rsquare <- length(nsim)
## Store bulk ESS for each parameter of each cumulative logit for each analysis model
bulkess_normal_largesd <- bulkess_normal_smallsd <- bulkess_laplace_largesd <- bulkess_laplace_smallsd <- bulkess_cauchy <- bulkess_rsquare <- length(nsim)
## Store tail ESS for each parameter of each cumulative logit for each analysis model
tailess_normal_largesd <- tailess_normal_smallsd <- tailess_laplace_largesd <- tailess_laplace_smallsd <- tailess_cauchy <- tailess_rsquare <- length(nsim)
## Store MCSE for each parameter of each cumulative logit for each analysis model
mcse_normal_largesd <- mcse_normal_smallsd <- mcse_laplace_largesd <- mcse_laplace_smallsd <- mcse_cauchy <- mcse_rsquare <- length(nsim)
## Number of divergent transitions
numdivergent_normal_largesd <- numdivergent_normal_smallsd <- numdivergent_laplace_largesd <- numdivergent_laplace_smallsd <- numdivergent_cauchy <- numdivergent_rsquare <- length(nsim)
x <- big_data$x[big_data$k == 1]
y <- as.integer(big_data$y[big_data$k == 1])
## Running the PO model
n_states = 4 ## Number of levels in ordinal outcome
n_patients = as.integer(length(big_data$y)) # Sample size
sdbeta = 100
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
## METHOD 1 assuming normal prior with SD = 100
example_fit <- stan(file = "po_model_normal.stan", data = example_data,
iter = 5000, chains = 4, seed = 22052024, cores = 4#,
# control = list(adapt_delta = 0.99, max_treedepth = 12)
)
table(x,y)
n_patients
## Running the PO model
n_states = 4 ## Number of levels in ordinal outcome
n_patients = as.integer(length(y)) # Sample size
sdbeta = 100
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
## METHOD 1 assuming normal prior with SD = 100
example_fit <- stan(file = "po_model_normal.stan", data = example_data,
iter = 1000, chains = 1, seed = 22052024, cores = 4#,
# control = list(adapt_delta = 0.99, max_treedepth = 12)
)
df <- as_draws_df(example_fit)
print(example_fit)
df
df <- df[, "beta"]
df
as.numeric(summarise_draws(df, "median")[7,2]) - log(1.50)
as.numeric(summarise_draws(df, "median")[7,2])
as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50)
as.numeric(summarise_draws(df, "median")[1,2])
as.numeric(summarise_draws(df, "median")[1,2])
as.numeric(summarise_draws(df, "median")[1,2])
lci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,2])
uci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,3])
lci
uci
ifelse(lci <= log(1.50) & uci >= log(1.50), 1, 0)
(as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50))^2
(as.numeric(summarise_draws(df, "median")[1,2])
fds
as.numeric(summarise_draws(df, "median")[1,2])
summarise_draws(df)
as.numeric(summarise_draws(df, "mcse_median")[1,2])
get_num_divergent(stanGet(example_fit))
library("rstan")
library('posterior')
library(rstanarm)
get_num_divergent(stanGet(example_fit))
library(rmsb)
get_num_divergent(stanGet(example_fit))
get_num_divergent(example_fit)
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), iter = 1000, cores = 4, seed = 22052024, chains = 1)
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1)
as.factor(y)
x
table(x,y)
example_fit <- stan_polr(y~x, prior = R2(0.001, "mean"), algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1)
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"))
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = example_data)
as.data.frame(example_data)
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data))
df <- as_draws_df(example_fit)
# Just care about log-OR
df <- df[, "beta"]
df
df <- df[, "x"]
as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50)
as.numeric(summarise_draws(df, "rhat")[1,2])
get_num_divergent(example_fit)
example_fit$stan_summary
get_num_divergent(example_fit$stan_summary)
get_num_divergent(df)
extract(example_fit)
as.stan(example_fit)
install.packages("pema")
library(pema)
get_num_divergent(as.stan(example_fit))
as.stan(example_fit)
as.stan(df)
rstan::get_sampler_params(example_fit, inc_warmup = FALSE)
library(rstan)
rstan::get_sampler_params(example_fit, inc_warmup = FALSE)
rstan::get_sampler_params(df, inc_warmup = FALSE)
example_fit <- stan_polr.fit(x,y, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 5000, cores = 4, seed = 22052024, chains = 4,
adapt_delta = 0.99, prior_counts = dirichlet(1))
example_fit <- stan_polr.fit(x,as.factor(y), prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 5000, cores = 4, seed = 22052024, chains = 4,
adapt_delta = 0.99, prior_counts = dirichlet(1))
as.data.frame(example_data)
example_fit <- stan_polr.fit(as.matrix(x),as.factor(y), prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 5000, cores = 4, seed = 22052024, chains = 4,
adapt_delta = 0.99, prior_counts = dirichlet(1))
get_num_divergent(stanGet(example_fit))
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 5000, cores = 4, seed = 22052024, chains = 4,
adapt_delta = 0.99, prior_counts = dirichlet(1), maxtreedepth = 12)
as_draws_df(example_fit)
df <- df[, "x"]
(as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50))^2
## METHOD 1 assuming normal prior with SD = 100
sdbeta = 100
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
example_fit <- stan(file = "po_model_normal.stan", data = example_data,
iter = 1000, chains = 4, seed = 22052024, cores = 4#,
# control = list(adapt_delta = 0.99, max_treedepth = 12)
)
df <- as_draws_df(example_fit)
(length(df$beta[df$beta > 0]))/4000
(length(df$beta[df$beta > 0]))/2000
(length(df$beta[df$beta > 0]))/2000
length(df$beta)
setwd("G:/BACKUP/Chris/Project_3/Main_simulations/Beta_prior/Small_n/Fixed/4_categories/Skewed/Moderate_effect")
## Main simulation scenario
rm(list=ls())
options(mc.cores = 8)
## Load in the appropriate packages
library("rstan")
library('posterior')
library('rstanarm')
args = commandArgs(trailingOnly = TRUE)
datnum <- as.numeric(args[1])
# Load the 1000 datasets
load("sim_data.Rdata")
attach(big_data)
# Select a subset of the data
s2 = seq(20,1000,by=20)
s1=s2-19
temp = big_data[(big_data$k >= s1[datnum] & big_data$k <= s2[datnum]),]
# Set seed
set.seed(071023)
seed=sample(1:1e8,50,replace=F)[datnum]
set.seed(seed)
nsim <- 1000
## Define states and other parameters
# 4 category outcome
states <- c("A", "B", "C","D")
#Generate empty vectors to store performance measures and diagnostic measures
## Store bias for each analysis model
bias_normal_largesd <- bias_normal_smallsd <- bias_laplace_largesd <- bias_laplace_smallsd <- bias_cauchy <- bias_rsquare <- length(nsim)
## Store coverage for each analysis model
coverage_normal_largesd <- coverage_normal_smallsd <- coverage_laplace_largesd <- coverage_laplace_smallsd <- coverage_cauchy <- coverage_rsquare <- length(nsim)
## Store MSE for each analysis model
mse_normal_largesd <- mse_normal_smallsd <- mse_laplace_largesd <- mse_laplace_smallsd <- mse_cauchy <- mse_rsquare <- length(nsim)
## Store posterior probability
postprob_normal_largesd <- postprob_normal_smallsd <- postprob_laplace_largesd <- postprob_laplace_smallsd <- postprob_cauchy <- postprob_rsquare <- length(nsim)
## Store R-hat for each parameter of each cumulative logit for each analysis model
rhat_normal_largesd <- rhat_normal_smallsd <- rhat_laplace_largesd <- rhat_laplace_smallsd <- rhat_cauchy <- rhat_rsquare <- length(nsim)
## Store bulk ESS for each parameter of each cumulative logit for each analysis model
bulkess_normal_largesd <- bulkess_normal_smallsd <- bulkess_laplace_largesd <- bulkess_laplace_smallsd <- bulkess_cauchy <- bulkess_rsquare <- length(nsim)
## Store tail ESS for each parameter of each cumulative logit for each analysis model
tailess_normal_largesd <- tailess_normal_smallsd <- tailess_laplace_largesd <- tailess_laplace_smallsd <- tailess_cauchy <- tailess_rsquare <- length(nsim)
## Store MCSE for each parameter of each cumulative logit for each analysis model
mcse_normal_largesd <- mcse_normal_smallsd <- mcse_laplace_largesd <- mcse_laplace_smallsd <- mcse_cauchy <- mcse_rsquare <- length(nsim)
## Number of divergent transitions
numdivergent_normal_largesd <- numdivergent_normal_smallsd <- numdivergent_laplace_largesd <- numdivergent_laplace_smallsd <- numdivergent_cauchy <- length(nsim)
x <- big_data$x[big_data$k == 1]
y <- as.integer(big_data$y[big_data$k == 1])
n_states = 4 ## Number of levels in ordinal outcome
n_patients = as.integer(length(y)) # Sample size
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.99, prior_counts = dirichlet(1), max_treedepth = 12)
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
## METHOD 1 assuming normal prior with SD = 100
sdbeta = 100
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1), max_treedepth = 12)
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1))
k <- 1
df <- as_draws_df(example_fit)
# Just care about log-OR
df <- df[, "x"]
# Extract summary for proportional OR
# Bias for each cumulative logit
bias_rsquare[k] <- as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50)
# Coverage - does the true value fall in the credible interval?
lci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,2])
uci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,3])
coverage_rsquare[k] <- ifelse(lci <= log(1.50) & uci >= log(1.50), 1, 0)
# MSE
mse_rsquare[k] <- (as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50))^2
# Posterior probability
postprob_rsquare[k] <- (length(df$beta[df$beta > 0]))/length(df$beta)
# MCSE
mcse_rsquare[k] <- as.numeric(summarise_draws(df, "mcse_median")[1,2])
# R-hat
rhat_rsquare[k] <- as.numeric(summarise_draws(df, "rhat")[1,2])
# Bulk ESS
bulkess_rsquare[k] <- as.numeric(summarise_draws(df, "ess_bulk")[1,2])
# Tail ESS
tailess_rsquare[k] <- as.numeric(summarise_draws(df, "ess_tail")[1,2])
}
postprob_rsquare
df$x
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.001, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1))
df <- as_draws_df(example_fit)
# Just care about log-OR
df <- df[, "x"]
# Extract summary for proportional OR
# Bias for each cumulative logit
bias_rsquare[k] <- as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50)
# Coverage - does the true value fall in the credible interval?
lci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,2])
uci <- as.numeric(summarise_draws(df, function(x) quantile(x, probs = c(0.025, 0.975)))[1,3])
coverage_rsquare[k] <- ifelse(lci <= log(1.50) & uci >= log(1.50), 1, 0)
# MSE
mse_rsquare[k] <- (as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50))^2
# Posterior probability
postprob_rsquare[k] <- (length(df[df$x > 0]))/length(df$x)
df[df$x > 0]
df$x
df$x > 0
df[df$x > 0]
length(df$x > 0)
length(df$x)
df$x > 0
df[df$x > 0]
df[df$x > 0,]
(length(df[df$x > 0,]))/length(df$x)
(length(df[df$x > 0,]))/length(df$x)
length(df[df$x > 0,])
df[df$x > 0,]
length(df[df$x > 0,])
dim(df[df$x > 0,])
dim(df[df$x > 0,])[1]
(dim(df[df$x > 0,])[1])/length(df$x)
load("G:/BACKUP/Chris/Project_3/Main_simulations/Beta_prior/Small_n/Fixed/4_categories/Skewed/Moderate_effect/sim_performance1.Rdata")
View(data)
mean(bias_normal_largesd)
mean(data$bias_normal_largesd)
mean(data$bias_normal_smallsd)
mean(data$bias_cauchy)
setwd("/group/cebu1/BACKUP/Chris/Project_3/Main_simulations/Beta_prior/Small_n/Fixed/4_categories/Skewed/Moderate_effect")
# Load the 1000 datasets
load("sim_data.Rdata")
attach(big_data)
x <- big_data$x[big_data$k == 1]
y <- as.integer(big_data$y[big_data$k == 1])
n_states = 4 ## Number of levels in ordinal outcome
n_patients = as.integer(length(y)) # Sample size
## METHOD 1 assuming normal prior with SD = 100
sdbeta = 100
example_data <- list(
y = y, ## Outcome
n_states = n_states, # Number of categories
n_patients = n_patients, ## Sample size
x = x, ## Treatment variable
p_par = rep(1, n_states), ## Concentration parameter for prior on alpha
sdbeta = sdbeta ## Specify prior SD for log-OR
)
table(x,y)
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.01, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1))
library(rstanarm)
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.01, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1))
df <- as_draws_df(example_fit)
df
df <- df[, "x"]
df
as.numeric(summarise_draws(df, "median")[1,2])
library(posterior)
as.numeric(summarise_draws(df, "median")[1,2])
log(1.5)
summarise_draws(df, "median")
summarise_draws(df)
warnings(example_fit)
## METHOD 6 assuming prior on the R-squared
example_fit <- stan_polr(as.factor(y)~x, prior = R2(0.1, "mean"), data = as.data.frame(example_data),
algorithm = "sampling", iter = 1000, cores = 4, seed = 22052024, chains = 1,
adapt_delta = 0.8, prior_counts = dirichlet(1))
df <- as_draws_df(example_fit)
df
as.numeric(summarise_draws(df, "median")[1,2]) - log(1.50)
as.numeric(summarise_draws(df, "median")[1,2])
exp(0.22566)
polr(as.factor(y)~x, data = as.data.frame(example_data),
Hess = TRUE)
# Frequentist analysis
library(MASS)
polr(as.factor(y)~x, data = as.data.frame(example_data),
Hess = TRUE)
freqmod <- polr(as.factor(y)~x, data = as.data.frame(example_data),
Hess = TRUE)
summary(freqmod)
freqmod$coefficients
confint(freqmod)
as.numeric(freqmod$coefficients)
as.numeric(freqmod$coefficients) - log(1.50)
confint(freqmod)[1]
as.numeric(confint(freqmod)[1])
(as.numeric(freqmod$coefficients) - log(1.50))^2
ifelse(as.numeric(confint(freqmod)[1]) <= log(1.50) & as.numeric(confint(freqmod)[2]) >= log(1.50), 1, 0)
